{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Container Shipment Order Management Service Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to container shipment order management service. It is part of the Event Driven Architecture solution implementation reference architecture. From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipment order issued by a manufacturer who want to ship fresh goods overseas. The business process is defined here . One of the business requirements for adopting event sourcing and CQRS patterns is to be able to get visibility to the history of orders and track the good shipment progress over time. This would include the ability to determine: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a container to pick up location? To answer those questions we need to keep historical information of each orders and its events. Event sourcing is to pattern of choice for that. We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach. Implementation approach As introduced in the solution high level design note the order entity life cycle looks like in the following diagram: The order microservice supports the implementations of this life cycle, using event sourcing and CQRS pattern. With CQRS , we separate the 'write model' from the 'read model'. The Command microservice implements the 'write model' and exposes a set of REST end points for Creating Order, Updating, Deleting Order and getting Order per ID. The query service will address more complex queries to support adhoc business requirements and most likely will need to join data between different entities like the order, the containers and the voyages. So we have two Java projects to support each service implementation. Each service is packaged as container and deployable to Kubernetes. Order command microservice Order query microservice As some requirements are related to historical query, using an event approach, we need to keep all the events related to what happens to the order. Instead of implementing a complex logic with the query and command services the event sourcing is supported by using Kafka topics. The following diagram illustrates the CQRS and event sourcing applied to the order management service. Client to the REST api, like a back end for front end app, performs a HTTP POST operation with the order data. The command generates events and persists order on its own data source. The query part is an event consumer and defines its own data projections to support the different queries: The datasource at the command level, may not be necessary, but we want to illustrate here the fact that it is possible to have a SQL based database or a document oriented database to keep the order last state: a call to get /orders/{id} will return the current order state. For the query part the projection can be kept in memory or persisted on its own data store. The decision, to go for in memory or to use a database, depends upon the amount of data to join, and the persitence time horizon set at the Kafka topic level. In case of problem or while starting, an event driven service may always rebuild its view by re-reading the topic from the beginning. Note An alternate solution is to have the BFF pushing events to the event source and then having the order service consuming event to persist them, as illustrated in the following diagram: As the BFF still needs to get order by ID or perform complex queries, it has to access the order service using HTTP, therefore we have prefered to use one communication protocol. The following sequence diagram illustrates the relationships between the components over time: To avoid transaction between the database update and the event published, the choice is to publish the event as early as it is received and use a consumer inside the command service to load the data and save to the database. The kafka topic act as a source of trust for this service. This is illustrated in this article. The /orders POST REST end point source code is here and the order events consumer in the command pattern. See the class OrderCRUDService.java . Produce order events to the orders topic. Consume events to update the state of the order or enrich it with new elements. When the application starts there is a ServletContextListener implementation class started to create a kafka consumer and to subscribe to order events (different types). When consumer reaches an issue to get event it creates an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop. Data and Event Model By applying a domain-driven design we can identify aggregates, entities, value objects and domain events. Those elements help us to be our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may look use relational database and may have information like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format pickupDate : string ; // date as ISO format } class OrderContainers { orderID : string ; containerID : string []; } On the event side we may generate OrderCreated, OrderCancelled,... But what is in the event payload? We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEvent { orderId : string ; timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. There are other questions we need to address is real project: do we need to ensure consistency between those data views? Can we consider the event, which are immutable data elements, as the source of truth? In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard. If you want to read more about the Event Sourcing and CQRS patterns see this article.","title":"Implementation considerations"},{"location":"#reefer-container-shipment-order-management-service","text":"Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to container shipment order management service. It is part of the Event Driven Architecture solution implementation reference architecture. From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipment order issued by a manufacturer who want to ship fresh goods overseas. The business process is defined here . One of the business requirements for adopting event sourcing and CQRS patterns is to be able to get visibility to the history of orders and track the good shipment progress over time. This would include the ability to determine: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a container to pick up location? To answer those questions we need to keep historical information of each orders and its events. Event sourcing is to pattern of choice for that. We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach.","title":"Reefer Container Shipment Order Management Service"},{"location":"#implementation-approach","text":"As introduced in the solution high level design note the order entity life cycle looks like in the following diagram: The order microservice supports the implementations of this life cycle, using event sourcing and CQRS pattern. With CQRS , we separate the 'write model' from the 'read model'. The Command microservice implements the 'write model' and exposes a set of REST end points for Creating Order, Updating, Deleting Order and getting Order per ID. The query service will address more complex queries to support adhoc business requirements and most likely will need to join data between different entities like the order, the containers and the voyages. So we have two Java projects to support each service implementation. Each service is packaged as container and deployable to Kubernetes. Order command microservice Order query microservice As some requirements are related to historical query, using an event approach, we need to keep all the events related to what happens to the order. Instead of implementing a complex logic with the query and command services the event sourcing is supported by using Kafka topics. The following diagram illustrates the CQRS and event sourcing applied to the order management service. Client to the REST api, like a back end for front end app, performs a HTTP POST operation with the order data. The command generates events and persists order on its own data source. The query part is an event consumer and defines its own data projections to support the different queries: The datasource at the command level, may not be necessary, but we want to illustrate here the fact that it is possible to have a SQL based database or a document oriented database to keep the order last state: a call to get /orders/{id} will return the current order state. For the query part the projection can be kept in memory or persisted on its own data store. The decision, to go for in memory or to use a database, depends upon the amount of data to join, and the persitence time horizon set at the Kafka topic level. In case of problem or while starting, an event driven service may always rebuild its view by re-reading the topic from the beginning. Note An alternate solution is to have the BFF pushing events to the event source and then having the order service consuming event to persist them, as illustrated in the following diagram: As the BFF still needs to get order by ID or perform complex queries, it has to access the order service using HTTP, therefore we have prefered to use one communication protocol. The following sequence diagram illustrates the relationships between the components over time: To avoid transaction between the database update and the event published, the choice is to publish the event as early as it is received and use a consumer inside the command service to load the data and save to the database. The kafka topic act as a source of trust for this service. This is illustrated in this article. The /orders POST REST end point source code is here and the order events consumer in the command pattern. See the class OrderCRUDService.java . Produce order events to the orders topic. Consume events to update the state of the order or enrich it with new elements. When the application starts there is a ServletContextListener implementation class started to create a kafka consumer and to subscribe to order events (different types). When consumer reaches an issue to get event it creates an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop.","title":"Implementation approach"},{"location":"#data-and-event-model","text":"By applying a domain-driven design we can identify aggregates, entities, value objects and domain events. Those elements help us to be our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may look use relational database and may have information like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format pickupDate : string ; // date as ISO format } class OrderContainers { orderID : string ; containerID : string []; } On the event side we may generate OrderCreated, OrderCancelled,... But what is in the event payload? We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEvent { orderId : string ; timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. There are other questions we need to address is real project: do we need to ensure consistency between those data views? Can we consider the event, which are immutable data elements, as the source of truth? In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard. If you want to read more about the Event Sourcing and CQRS patterns see this article.","title":"Data and Event Model"},{"location":"build-run/","text":"Build and run the order microsercives locally We support different deployment models: local with docker-compose, local with minikube, and remote using kubernetes on IBM Cloud (IKS), or on IBM Cloud private. To build and run we are proposing some scripts. Each script accepts an argument: LOCAL (default is argument is omitted) MINIKUBE IBM_CLOUD ICP This argument is really used to set environment variables used in the code in a separate script under the refarch-kc project and the scripts folder. Pre-requisites You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux) Build You need to build each microservices independently using maven. Each microservice has its ownbuild script to perform the maven package and build the docker image. See scripts folder under each project. For order-command-ms cd order-command-ms ./scripts/buildDocker.sh MINIKUBE For order-query-ms ./scripts/buildDocker.sh MINIKUBE Note The build scripts test if the javatool docker image exists and they use it, if found, otherwie they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase/kc-orderqueryms latest b85b43980f35 531MB ibmcase/kc-ordercommandms latest Run You can always use the maven command to compile and run liberty server for each project. mvn install mvn liberty:run-server But as soon as you need to run integration tests with kafka you need all services up and running. On Minikube For the order command microservice: cd order-command-ms helm install chart/ordercommandms/ --name ordercmd --set image.repository=ibmcase/kc-ordercommandms --set image.pullSecret= --set image.pullPolicy=Never --set eventstreams.brokers=kafkabitmani:9092 --set eventstreams.env=MINIKUBE --namespace greencompute or use the command: ./scripts/deployHelm MINIKUBE Without any previously tests done, the call below should return an empty array: [] curl http://localhost:31200/orders We will present some integration tests in a section below. With docker compose To run the complete solution locally we use docker compose from the root project. And to stop everything: docker-compose -f kc-solution-compose.yml down docker-compose -f backbone-compose.yml down","title":"Build and run locally"},{"location":"build-run/#build-and-run-the-order-microsercives-locally","text":"We support different deployment models: local with docker-compose, local with minikube, and remote using kubernetes on IBM Cloud (IKS), or on IBM Cloud private. To build and run we are proposing some scripts. Each script accepts an argument: LOCAL (default is argument is omitted) MINIKUBE IBM_CLOUD ICP This argument is really used to set environment variables used in the code in a separate script under the refarch-kc project and the scripts folder.","title":"Build and run the order microsercives locally"},{"location":"build-run/#pre-requisites","text":"You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work. Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) , or Download a Liberty server package that contains the IBM JDK (Windows, Linux)","title":"Pre-requisites"},{"location":"build-run/#build","text":"You need to build each microservices independently using maven. Each microservice has its ownbuild script to perform the maven package and build the docker image. See scripts folder under each project. For order-command-ms cd order-command-ms ./scripts/buildDocker.sh MINIKUBE For order-query-ms ./scripts/buildDocker.sh MINIKUBE Note The build scripts test if the javatool docker image exists and they use it, if found, otherwie they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase/kc-orderqueryms latest b85b43980f35 531MB ibmcase/kc-ordercommandms latest","title":"Build"},{"location":"build-run/#run","text":"You can always use the maven command to compile and run liberty server for each project. mvn install mvn liberty:run-server But as soon as you need to run integration tests with kafka you need all services up and running.","title":"Run"},{"location":"build-run/#on-minikube","text":"For the order command microservice: cd order-command-ms helm install chart/ordercommandms/ --name ordercmd --set image.repository=ibmcase/kc-ordercommandms --set image.pullSecret= --set image.pullPolicy=Never --set eventstreams.brokers=kafkabitmani:9092 --set eventstreams.env=MINIKUBE --namespace greencompute or use the command: ./scripts/deployHelm MINIKUBE Without any previously tests done, the call below should return an empty array: [] curl http://localhost:31200/orders We will present some integration tests in a section below.","title":"On Minikube"},{"location":"build-run/#with-docker-compose","text":"To run the complete solution locally we use docker compose from the root project. And to stop everything: docker-compose -f kc-solution-compose.yml down docker-compose -f backbone-compose.yml down","title":"With docker compose"},{"location":"ddd-applied/","text":"Domain-driven design applied to order context During the event storming analysis, we define the domain to be the container shipment domain. It groups a set of subdomains like orders, contract, shipping, and external systems as the voyage scheduling and the container inventory management: Note Notice that at this time just three physical systems exist. The grouping of the orders, contract and shipping sub domain in one boundary context may be considered as an analysis shortcut as we want to clearly separate those subdomain as the ownership and the ubiquituous language are differents. We have three subdomain and all can be considered core domains. They represent company's competitive advantages and directly impact the organization business. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel voyages. Voyage is an entity grouping the information about the source harbor close to the pickup destination, to a target harbor the closest to the shipping destination. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor. Bounded Contexts Within a business context every use of a given domain term, phrase, or sentence, the Ubiquitous Language inside the boundary has a specific contextual meaning. So order context is a boundary context and groups order, ordered product type, pickup and shipping addresses. The business problem to address is, how to make order traceability more efficient so customers have a clear view of their orders. An order will be assigned to one or many containers and containers are assigned to a voyage. Order aggregate In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction. Command - Policies and Events When adding commands and business policies to the discovered events, we were able to isolate the following command and events for the order context. From the command we can design our first APIs interface: Order createOrder(orderDTO) Order updateOrder(order) Order getOrders(manufacturerName) User stories The business requirements is presented in this note The following user stories are implemented in this project: [ ] As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. [ ] As a shipping company manager, I want to get a specific order list knowing its unique identifier so that I can review the data and know the current status. [ ] As a shipping company manager, I want to update the status of an order [ ] As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. [ ] As a shipping company manager, I want to review the containers allocated to the order because I'm curious [ ] As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations.","title":"From analysis to implementation"},{"location":"ddd-applied/#domain-driven-design-applied-to-order-context","text":"During the event storming analysis, we define the domain to be the container shipment domain. It groups a set of subdomains like orders, contract, shipping, and external systems as the voyage scheduling and the container inventory management: Note Notice that at this time just three physical systems exist. The grouping of the orders, contract and shipping sub domain in one boundary context may be considered as an analysis shortcut as we want to clearly separate those subdomain as the ownership and the ubiquituous language are differents. We have three subdomain and all can be considered core domains. They represent company's competitive advantages and directly impact the organization business. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel voyages. Voyage is an entity grouping the information about the source harbor close to the pickup destination, to a target harbor the closest to the shipping destination. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor.","title":"Domain-driven design applied to order context"},{"location":"ddd-applied/#bounded-contexts","text":"Within a business context every use of a given domain term, phrase, or sentence, the Ubiquitous Language inside the boundary has a specific contextual meaning. So order context is a boundary context and groups order, ordered product type, pickup and shipping addresses. The business problem to address is, how to make order traceability more efficient so customers have a clear view of their orders. An order will be assigned to one or many containers and containers are assigned to a voyage.","title":"Bounded Contexts"},{"location":"ddd-applied/#order-aggregate","text":"In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction.","title":"Order aggregate"},{"location":"ddd-applied/#command-policies-and-events","text":"When adding commands and business policies to the discovered events, we were able to isolate the following command and events for the order context. From the command we can design our first APIs interface: Order createOrder(orderDTO) Order updateOrder(order) Order getOrders(manufacturerName)","title":"Command - Policies and Events"},{"location":"ddd-applied/#user-stories","text":"The business requirements is presented in this note The following user stories are implemented in this project: [ ] As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. [ ] As a shipping company manager, I want to get a specific order list knowing its unique identifier so that I can review the data and know the current status. [ ] As a shipping company manager, I want to update the status of an order [ ] As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. [ ] As a shipping company manager, I want to review the containers allocated to the order because I'm curious [ ] As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations.","title":"User stories"},{"location":"deployments/","text":"Deployments Be sure to have read the build and run article before . Deploy to IKS Be sure to have read and done the steps to prepare the IBM Cloud services and get a kubernetes cluster up and running. Once the two docker images are built, upload them to the IKS private registry docker push us.icr.io/ibmcaseeda/kc-orderqueryms docker push us.icr.io/ibmcaseeda/kc-ordercmdms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order-command-ms ./scripts/deployHelm MINIKUBE cd order-query-ms ./scripts/deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h Deploy to ICP","title":"Kubernetes Deployments"},{"location":"deployments/#deployments","text":"Be sure to have read the build and run article before .","title":"Deployments"},{"location":"deployments/#deploy-to-iks","text":"Be sure to have read and done the steps to prepare the IBM Cloud services and get a kubernetes cluster up and running. Once the two docker images are built, upload them to the IKS private registry docker push us.icr.io/ibmcaseeda/kc-orderqueryms docker push us.icr.io/ibmcaseeda/kc-ordercmdms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order-command-ms ./scripts/deployHelm MINIKUBE cd order-query-ms ./scripts/deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h","title":"Deploy to IKS"},{"location":"deployments/#deploy-to-icp","text":"","title":"Deploy to ICP"}]}