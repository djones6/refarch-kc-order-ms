{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reefer Container Shipment Order Management Service Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to Reefer shipment order subdomain. It is part of the Event Driven Architecture solution implementation . From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipping order issued by a customer who wants to ship fresh goods overseas. The business process is defined here and the event storming analysis in this note . We are also presenting one way of applying Domain Drive Design practice for this subdomain. What you will learn By studying this repository you will be able to learn the following subjects: How to apply domain driven design for a CQRS microservice How to adopt CQRS pattern for the shipping order management How to apply ubiquituous language in the code Develop and deploy a microprofile 2.2 application, using open Liberty, on openshift or kubernetes Requirements The key business requirements we need to support are: Be able to book a fresh product shipment order, including the allocation of the voyage and the assignment of a reefer container to the expected cargo. Be able to understand what happen to the order over time: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? Track key issue or step in the reefer shipment process How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? Be able to support adhoc query on the order that span across subdomains of the shipment domain. What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a fresh food order from california to China? Those requirements force use to consider event sourcing (understanding facts about the order over time) and CQRS patterns to separate queries from command so our architecture will be more flexible and may address different scaling requirements. Applying Domain Driven Design We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach. Review detail implementation considerations The code is the source of truth, but we are providing some simple explanations on how to navigate into the code, and some implementation consideration in this note . Deploy to kubernetes See this note on how to deploy this service with its configuration on Openshift or kubernetes.","title":"Introduction"},{"location":"#reefer-container-shipment-order-management-service","text":"Abstract This project is demonstrating, one of the possible implementation of the Command Query Responsibility Segregation and event sourcing patterns applied to Reefer shipment order subdomain. It is part of the Event Driven Architecture solution implementation . From a use case point of view, it implements the order management component, responsible to manage the full life cycle of a shipping order issued by a customer who wants to ship fresh goods overseas. The business process is defined here and the event storming analysis in this note . We are also presenting one way of applying Domain Drive Design practice for this subdomain.","title":"Reefer Container Shipment Order Management Service"},{"location":"#what-you-will-learn","text":"By studying this repository you will be able to learn the following subjects: How to apply domain driven design for a CQRS microservice How to adopt CQRS pattern for the shipping order management How to apply ubiquituous language in the code Develop and deploy a microprofile 2.2 application, using open Liberty, on openshift or kubernetes","title":"What you will learn"},{"location":"#requirements","text":"The key business requirements we need to support are: Be able to book a fresh product shipment order, including the allocation of the voyage and the assignment of a reefer container to the expected cargo. Be able to understand what happen to the order over time: How frequently does an order get cancelled after it is placed but before an empty container is delivered to pick up location or loaded ? Track key issue or step in the reefer shipment process How often does an order get cancelled after the order is confirmed, a container assigned and goods loaded into it? Be able to support adhoc query on the order that span across subdomains of the shipment domain. What are all events for a particular order and associated container shipment? Has the cold chain been protected on this particular order? How long it takes to deliver a fresh food order from california to China? Those requirements force use to consider event sourcing (understanding facts about the order over time) and CQRS patterns to separate queries from command so our architecture will be more flexible and may address different scaling requirements.","title":"Requirements"},{"location":"#applying-domain-driven-design","text":"We are detailing, in a separate note , how to go from the event storming produced elements to the microservice implementation by applying the domain-driven design approach.","title":"Applying Domain Driven Design"},{"location":"#review-detail-implementation-considerations","text":"The code is the source of truth, but we are providing some simple explanations on how to navigate into the code, and some implementation consideration in this note .","title":"Review detail implementation considerations"},{"location":"#deploy-to-kubernetes","text":"See this note on how to deploy this service with its configuration on Openshift or kubernetes.","title":"Deploy to kubernetes"},{"location":"build-run/","text":"Build and run the order microsercives locally We support different deployment models: local with docker-compose, and remote using kubernetes on IBM Cloud (IKS), or on premise with Openshift. To build and run we are proposing to use some shell scripts. Each script accepts one argument: LOCAL (default is argument is omitted) IBM_CLOUD This argument is used to set environment variables used in the code. In fact the setenv.sh script is defined in the root refarch-kc project and the scripts folder. Pre-requisites You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work like: Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) For IBMCLOUD, you need to be sure to have an Event Stream service defined (See this note for a simple summary of what to do). If you run IBM Event Streams on openshift cluster on premise servers, be sure to get truststore certificates and API key. Configure the following topics for both microservices: orderCommands , errors , orders . You can use the script createTopics.sh in the refarch-kc project for that or use the Event Streams user interface. Build There are two separate folder to manage the code and scripts for the CQRS command and query part: order-command-ms order-query-ms You need to build each microservice independently using maven. Each microservice has its own build script to perform the maven package and build the docker image. See scripts folder under each project. Any microservice in this repository can be compiled, unit tested and packaged as war file using maven: mvn package For order-command-ms, the following command will run unit tests and package the war file, then build a docker image cd order-command-ms ./scripts/buildDocker.sh IBMCLOUD If you want to run the integration test you need to do the following: source ../../refarch-kc/scripts/setenv.sh IBMCLOUD mvn install or mvn integration-test For order-query-ms ./scripts/buildDocker.sh IBMCLOUD Note The build scripts test if the javatool docker image exists and if so they use it, otherwise they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase/kc-orderqueryms latest b85b43980f35 531MB ibmcase/kc-ordercommandms latest Run You can always use the maven command to compile and run liberty server for each project. Before doing so be sure to have set the KAFKA_BROKERS and KAFKA_API_KEY environment variables with the setenv.sh command coming from the refarch-kc project, which should be at the same level in folder hierarchy as this repository. source ../../refarch-kc/scripts/setenv.sh IBMCLOUD mvn install mvn liberty:run-server With docker compose To run the complete solution locally we use docker compose from the root refarch-kc project. docker-compose -f backbone-compose.yml up docker-compose -f kc-solution-compose.yml up And to stop everything: docker-compose -f kc-solution-compose.yml down docker-compose -f backbone-compose.yml down Deploy on kubernetes cluster See this note.","title":"Build and run locally"},{"location":"build-run/#build-and-run-the-order-microsercives-locally","text":"We support different deployment models: local with docker-compose, and remote using kubernetes on IBM Cloud (IKS), or on premise with Openshift. To build and run we are proposing to use some shell scripts. Each script accepts one argument: LOCAL (default is argument is omitted) IBM_CLOUD This argument is used to set environment variables used in the code. In fact the setenv.sh script is defined in the root refarch-kc project and the scripts folder.","title":"Build and run the order microsercives locally"},{"location":"build-run/#pre-requisites","text":"You can have the following software already installed on your computer or use our docker images to get those dependencies integrated in docker images, which you can use to build, test and package the java programs. Maven Java 8: Any compliant JVM should work like: Java 8 JDK from Oracle Java 8 JDK from IBM (AIX, Linux, z/OS, IBM i) For IBMCLOUD, you need to be sure to have an Event Stream service defined (See this note for a simple summary of what to do). If you run IBM Event Streams on openshift cluster on premise servers, be sure to get truststore certificates and API key. Configure the following topics for both microservices: orderCommands , errors , orders . You can use the script createTopics.sh in the refarch-kc project for that or use the Event Streams user interface.","title":"Pre-requisites"},{"location":"build-run/#build","text":"There are two separate folder to manage the code and scripts for the CQRS command and query part: order-command-ms order-query-ms You need to build each microservice independently using maven. Each microservice has its own build script to perform the maven package and build the docker image. See scripts folder under each project. Any microservice in this repository can be compiled, unit tested and packaged as war file using maven: mvn package For order-command-ms, the following command will run unit tests and package the war file, then build a docker image cd order-command-ms ./scripts/buildDocker.sh IBMCLOUD If you want to run the integration test you need to do the following: source ../../refarch-kc/scripts/setenv.sh IBMCLOUD mvn install or mvn integration-test For order-query-ms ./scripts/buildDocker.sh IBMCLOUD Note The build scripts test if the javatool docker image exists and if so they use it, otherwise they use maven. If you want to use docker compose use LOCAL as parameter. Verify the docker images are created docker images ibmcase/kc-orderqueryms latest b85b43980f35 531MB ibmcase/kc-ordercommandms latest","title":"Build"},{"location":"build-run/#run","text":"You can always use the maven command to compile and run liberty server for each project. Before doing so be sure to have set the KAFKA_BROKERS and KAFKA_API_KEY environment variables with the setenv.sh command coming from the refarch-kc project, which should be at the same level in folder hierarchy as this repository. source ../../refarch-kc/scripts/setenv.sh IBMCLOUD mvn install mvn liberty:run-server","title":"Run"},{"location":"build-run/#with-docker-compose","text":"To run the complete solution locally we use docker compose from the root refarch-kc project. docker-compose -f backbone-compose.yml up docker-compose -f kc-solution-compose.yml up And to stop everything: docker-compose -f kc-solution-compose.yml down docker-compose -f backbone-compose.yml down","title":"With docker compose"},{"location":"build-run/#deploy-on-kubernetes-cluster","text":"See this note.","title":"Deploy on kubernetes cluster"},{"location":"ddd-applied/","text":"The event storming workshop helped us to engage with the business experts and understand the end to end reefer shipping process as well as the events created. The following diagram represents part of this work: (It may have evolved a little bit as we worked with domain experts to learn about the domain) We assume you are now familiar with the event storming methodology as presented in this note . And you have some basic knowledge of domain driven design, from reading books like Eric Evans's \"Domain Driven Design: Tackling Complexity in the Heart of Software\" book . We recommend to read the chapter \"From analysis to implementation\" to get a clear understanding of the application design and the ubiquituous language of the reefer shipping domain. In this part we focus on the order sub-domain. The order subdomain interacts with the contract subdomain via the acceptance of the contract conditions from the customer (e.g. manufacturer) and by building a contract from the created shipment order. When the contract is accepted, the order needs to be shipped, but to do so the shipping subdomain needs to interact with the voyage subsystem to get the available vessel itineraries. Voyage is an entity grouping the information about the origin harbor close to the pickup address, to a destination harbor the closest to the shipping address. Finally, the shipping subdomain needs to interact with the container inventory service to get matching Reefer containers, present at the source harbor. Ubiquitous language Entities and Value Objects Order is the main business entity of this service, and is uniquely identified by its orderID. The orderID is sequentially created when persisting it in its repository. The OrderID will be communicated asynchronously to the customer via a confirmation email. The value objects: the different addresses delivery specifications deliver history Aggregate boundaries In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction. The Reefer and Order and Voyage seem to be an obvious aggregates. Customer and Vessets are also aggregates, but we will not consider them as scope for the solution implementation. In this project we focus on the shipping order aggregate. Shipment order lifecycle and state change events The scoping decisions for the demonstration build listed above are reflected in a shipment order life cycle diagram shown below. A shipment order is initially created with an API call made by a manufacturer, or via a user interface The order request specifies: The pickup location where empty container will be loaded The delivery location where the container is to be delivered to (we expect this to be in a remote country requiring a sea voyage) The type of good with the target temperature to maintain a long the voyage. The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required Since our initial demonstration build expects to show refrigeration behavior and track preservation of a cold chain, we assume that orders are for some commodity which requires refrigeration during its shipment. When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the manufacturer / customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location then the order can transition to state=BOOKED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester. Once an order is BOOKED , then the expected dates and locations where for which a container will be needed are known. A request can be issued to book a specific (refrigerated) container for use with this shipment. We assume that the shipping company always has enough container available to meet expected shipment demand, hence the shipment order will transition to state=CONTAINER_ALLOCATED when this container booking is received. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=FULL_CONTAINER_VOYAGE_READY . The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state = FULL_CONTAINER_VOYAGE_READY to state = CONTAINER_ON_SHIP . While the order has state = CONTAINER_ON_SHIP , then we will be receiving temperature information from the Container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming souces of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED . Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED . We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state = REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation. Repositories The Order aggregate has its own repository. Command - Policies and Events When adding commands and business policies to the discovered events, we were able to isolate the following command and events for the order context. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window. CRQS and event sourcing It makes sense to use CQRS and separate out order tracking into a separate orders-query-ms since: The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API. User stories The business requirements is presented in this note The following user stories are implemented in this project: [ ] As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. [ ] As a shipping company manager, I want to get a specific order list knowing its unique identifier so that I can review the data and know the current status. [ ] As a shipping company manager, I want to update the status of an order [ ] As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. [ ] As a shipping company manager, I want to review the containers allocated to the order because I'm curious [ ] As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations.","title":"Order subdomain - analysis and design"},{"location":"ddd-applied/#ubiquitous-language","text":"","title":"Ubiquitous language"},{"location":"ddd-applied/#entities-and-value-objects","text":"Order is the main business entity of this service, and is uniquely identified by its orderID. The orderID is sequentially created when persisting it in its repository. The OrderID will be communicated asynchronously to the customer via a confirmation email. The value objects: the different addresses delivery specifications deliver history","title":"Entities and Value Objects"},{"location":"ddd-applied/#aggregate-boundaries","text":"In Domain-driven design, an aggregate groups related object as a unique entity. One object is the aggregate root. It ensures the integrity of the whole. Here the root is the Order. The product to ship is part of the aggregate. The Order is what we will persist in one unique transaction. The Reefer and Order and Voyage seem to be an obvious aggregates. Customer and Vessets are also aggregates, but we will not consider them as scope for the solution implementation. In this project we focus on the shipping order aggregate.","title":"Aggregate boundaries"},{"location":"ddd-applied/#shipment-order-lifecycle-and-state-change-events","text":"The scoping decisions for the demonstration build listed above are reflected in a shipment order life cycle diagram shown below. A shipment order is initially created with an API call made by a manufacturer, or via a user interface The order request specifies: The pickup location where empty container will be loaded The delivery location where the container is to be delivered to (we expect this to be in a remote country requiring a sea voyage) The type of good with the target temperature to maintain a long the voyage. The shipment time window i.e.: Earliest date at which goods are available at pickup location for loading into the container Date by which delivery to the destination address is required Since our initial demonstration build expects to show refrigeration behavior and track preservation of a cold chain, we assume that orders are for some commodity which requires refrigeration during its shipment. When a new shipment order is placed, the shipping company must determine whether there is available capacity in some planned ship voyage which meets all the requirements specified by the manufacturer / customer. If there is a planned voyage with available capacity for additional container going from the source port nearest the pickup location to the destination port nearest to the delivery location then the order can transition to state=BOOKED and positive confirmation of the order returned to the requester. If no such voyage is available then the shipment order transitions to state=REJECTED (No Availability) and this is reported back to the requester. Once an order is BOOKED , then the expected dates and locations where for which a container will be needed are known. A request can be issued to book a specific (refrigerated) container for use with this shipment. We assume that the shipping company always has enough container available to meet expected shipment demand, hence the shipment order will transition to state=CONTAINER_ALLOCATED when this container booking is received. Since the scope for this demonstration build excluded the simulation of trucking operations to get the goods from the manufacturer's pickup location, export clearance and actual dockside loading operations, once an order has a container allocated it is \"ready to go\" and transitions to state=FULL_CONTAINER_VOYAGE_READY . The actual event of recording the container as being on board ship and at sea will not happen until simulated time in the demonstration reaches the scheduled start of the voyage on which that container is booked and the container ship assigned to that voyage is in the source port and also ready to go. At that point in simulated time, the state of the shipment order changes from state = FULL_CONTAINER_VOYAGE_READY to state = CONTAINER_ON_SHIP . While the order has state = CONTAINER_ON_SHIP , then we will be receiving temperature information from the Container simulation and Ship position information from the ship simulation service. Both provide a continuous streaming souces of information which should be considered part of the extended shipment state. After some period of simulated time, the ship will reach the destination port of the voyage. At this time the order transitions to state = CONTAINER_OFF_SHIP since our scope excluded simulation of actual dockside unloading information. Since we are not modelling customs clearance or trucking operations, there are no further events to be modeled until the order state = CONTAINER_DELIVERED . Since we are not modelling invoicing and billing operations the Container can be deallocated from this order and returned to some pool of free containers. When that has occurred the order state can be considered state = ORDER_COMPLETED . We have described the nornal, exception-free path first. There are two exception cases modelled: At the time a new shipment order is requested, there may be no voyage with available capacity meeting the location and time requirements of the request. When this occurs, the manufacturer/user is informed and the order state becomes state= REJECTED (No Availability). At this point, the user can modify the order with a second API requests changing dates or possibly locations. This retry request could still fail returning the order back to state = REJECTED ( No availability). Alternatively the changes in dates and location could be enough for an available voyage to be found. When this occurs the order will transition to state = BOOKED modified. If an API call to modify an order is made and the order is in some state different from state = REJECTED (No availability), we reject the API request. There could be race conditions, the order is in the process of being assigned to a voyage, or complex recovery issues. What if the order is already in a container and at sea when a modify order is received ? Full treatment of these complex business specific issues is out of scope and avoided by the state check in the modify order call API call We also model the exception condition when the refrigeration unit in a container fails or is misset or over loaded. If the temperature in the container goes outside the service level range for that shipment the goods must be considered spoiled. The order will transition from state = CONTAINER_ON_SHIP to state = ORDER_SPOILED (Temperature out of Range). Some complex business recovery such as compensating the customer and possibly scheduling a replacement shipment may be required. The details will be contract specific and outside the scope, but we do include the use of Streaming event container analytics to detect the spoilage and use rule based real-time /edge adjustments of the refrigeration gear to avoid spoilage in the demonstration simulation.","title":"Shipment order lifecycle and state change events"},{"location":"ddd-applied/#repositories","text":"The Order aggregate has its own repository.","title":"Repositories"},{"location":"ddd-applied/#command-policies-and-events","text":"When adding commands and business policies to the discovered events, we were able to isolate the following command and events for the order context. Command APIs will be provided to: Place a new shipment order. Track an existing order, to confirm its booking state or to resolve the actual location and status of the container in transit. Modify an order request which could not be booked within the requested time window.","title":"Command - Policies and Events"},{"location":"ddd-applied/#crqs-and-event-sourcing","text":"It makes sense to use CQRS and separate out order tracking into a separate orders-query-ms since: The demand for order tracking might have significantly more intense scalability needs than order commands. Orders are typically created once and changes state a handful of times. There could be many different users querying status of a particular orders independently and each requesting tracking multiple times for each order to determine if there is some delay expected. Order state tracking information should probably be organized by requesting customer NOT by order ID: since customers should be allowed to see status on their own orders but not on other customer's orders when the shipping company is tracking an order it is most frequently doing so on behalf of a specific customer With this approach orders-query-ms becomes a CQRS query service with internal state updated from the event backbone, and an order tracking API.","title":"CRQS and event sourcing"},{"location":"ddd-applied/#user-stories","text":"The business requirements is presented in this note The following user stories are implemented in this project: [ ] As a shipping company manager, I want to get the current newly created order list so that I can create contract manually. [ ] As a shipping company manager, I want to get a specific order list knowing its unique identifier so that I can review the data and know the current status. [ ] As a shipping company manager, I want to update the status of an order [ ] As a shipping company manager, I want select one of the proposed voyages, so that I can optimize the vessel allocation, and satisfy the customer. [ ] As a shipping company manager, I want to review the containers allocated to the order because I'm curious [ ] As a shipping company manager, I want to modify pickup date and expected delivery date to adapt to customer last request The selected voyage must be from a source port near the pickup location travelling to a destination port near the delivery location requested by the customer. It must be within the time window specified by the customer in the order request. The selected voyage must have free space available (capacity not previously assigned to other orders) to accomodate the number of containers specified by the customer in their shipment request. From a design point of view we can imagine there is an automatic system being able to perform this assignment. This is implemented in the Voyage microservice Warning All the end user interactions are done in the user interface, in a separate project, but the order microservice supports the backend operations.","title":"User stories"},{"location":"deployments/","text":"Deployments Be sure to have read the build and run article before . Deployment prerequisites Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider. Get the API key from the user interface using an administrator user. Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Note The name of those secrets are used in the Helm chart values.yaml and deployment.yaml files of each project. kafka : brokersConfigMap : kafka - brokers eventstreams : enabled : true apikeyConfigMap : eventstreams - apikey truststoreRequired : true truststorePath : /config/resources/security/ es - ssl truststoreFile : es - cert . jks truststoreSecret : es - truststore - jks truststorePassword : changeit Deploy to IKS Be sure to have created an IBM kubernetes service cluster (See this lab for detail) Once the two docker images are built, upload them to the IKS private registry docker push us.icr.io/ibmcaseeda/kc-orderqueryms docker push us.icr.io/ibmcaseeda/kc-ordercmdms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order-command-ms ./scripts/deployHelm MINIKUBE cd order-query-ms ./scripts/deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h Deploy to OpenShift Container Platform (OCP) Deploy to OCP 3.11 Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) If desired, create a non-default Service Account for usage of deploying and running the Reefer Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges. Perform the following for both order-command-ms and order-query-ms microservices: Build and push the Docker image by one of the two options below: Create a Jenkins project, pointing to the remote GitHub repository for the order-command and order-query microservices, and manually creating the necessary parameters. Refer to the order-command Jenkinsfile.NoKubernetesPlugin or order-query Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): docker build -t <private-registry>/<image-namespace>/order-command-ms:latest order-command-ms/ docker build -t <private-registry>/<image-namespace>/order-query-ms:latest order-query-ms/ docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template for both order-command and order-query : Parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (optional or set to blank) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set image.pullSecret = --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms Deploy application using oc apply : oc apply -f templates/ordercommandms/templates","title":"Deploy to Kubernetes cluster"},{"location":"deployments/#deployments","text":"Be sure to have read the build and run article before .","title":"Deployments"},{"location":"deployments/#deployment-prerequisites","text":"Regardless of specific deployment targets (OCP, IKS, k8s), the following prerequisite Kubernetes artifacts need to be created to support the deployments of application components. These artifacts need to be created once per unique deployment of the entire application and can be shared between application components in the same overall application deployment. Create kafka-brokers ConfigMap Command: kubectl create configmap kafka-brokers --from-literal=brokers='<replace with comma-separated list of brokers>' -n <namespace> Example: kubectl create configmap kafka-brokers --from-literal=brokers='broker-3-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-2-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-1-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-5-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-0-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093,broker-4-j7fxtxtp5fs84205.kafka.svc01.us-south.eventstreams.cloud.ibm.com:9093' -n eda-refarch Create optional eventstreams-apikey Secret, if you are using Event Streams as your Kafka broker provider. Get the API key from the user interface using an administrator user. Command: kubectl create secret generic eventstreams-apikey --from-literal=binding='<replace with api key>' -n <namespace> Example: kubectl create secret generic eventstreams-apikey --from-literal=binding='z...12345...notanactualkey...67890...a' -n eda-refarch If you are using Event Streams as your Kafka broker provider and it is deployed via the IBM Cloud Pak for Integration (ICP4I), you will need to create an additional Secret to store the generated Certificates & Truststores. From the \"Connect to this cluster\" tab on the landing page of your Event Streams installation, download both the Java truststore and the PEM certificate . Create the Java truststore Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.jks Example: kubectl create secret generic es-truststore-jks --from-file=/Users/osowski/Downloads/es-cert.jks Create the PEM certificate Secret: Command: kubectl create secret generic <secret-name> --from-file=/path/to/downloaded/file.pem Example: kubectl create secret generic es-ca-pemfile --from-file=/Users/osowski/Downloads/es-cert.pem Note The name of those secrets are used in the Helm chart values.yaml and deployment.yaml files of each project. kafka : brokersConfigMap : kafka - brokers eventstreams : enabled : true apikeyConfigMap : eventstreams - apikey truststoreRequired : true truststorePath : /config/resources/security/ es - ssl truststoreFile : es - cert . jks truststoreSecret : es - truststore - jks truststorePassword : changeit","title":"Deployment prerequisites"},{"location":"deployments/#deploy-to-iks","text":"Be sure to have created an IBM kubernetes service cluster (See this lab for detail) Once the two docker images are built, upload them to the IKS private registry docker push us.icr.io/ibmcaseeda/kc-orderqueryms docker push us.icr.io/ibmcaseeda/kc-ordercmdms Verify the images are in you private repo: ibmcloud cr image-list Deploy the helm charts for each service using the scripts/deployHelm under each project. cd order-command-ms ./scripts/deployHelm MINIKUBE cd order-query-ms ./scripts/deployHelm MINIKUBE Verify the deployments and pods: kubectl get deployments -n browncompute NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE fleetms-deployment 1 1 1 1 23h kc-ui 1 1 1 1 18h ordercommandms-deployment 1 1 1 1 1d orderqueryms-deployment 1 1 1 1 23h voyagesms-deployment 1 1 1 1 19h","title":"Deploy to IKS"},{"location":"deployments/#deploy-to-openshift-container-platform-ocp","text":"","title":"Deploy to OpenShift Container Platform (OCP)"},{"location":"deployments/#deploy-to-ocp-311","text":"Cross-component deployment prerequisites: (needs to be done once per unique deployment of the entire application) If desired, create a non-default Service Account for usage of deploying and running the Reefer Container reference implementation. This will become more important in future iterations, so it's best to start small: Command: oc create serviceaccount -n <target-namespace> kcontainer-runtime Example: oc create serviceaccount -n eda-refarch kcontainer-runtime The target Service Account needs to be allowed to run containers as anyuid for the time being: Command: oc adm policy add-scc-to-user anyuid -z <service-account-name> -n <target-namespace> Example: oc adm policy add-scc-to-user anyuid -z kcontainer-runtime -n eda-refarch NOTE: This requires cluster-admin level privileges. Perform the following for both order-command-ms and order-query-ms microservices: Build and push the Docker image by one of the two options below: Create a Jenkins project, pointing to the remote GitHub repository for the order-command and order-query microservices, and manually creating the necessary parameters. Refer to the order-command Jenkinsfile.NoKubernetesPlugin or order-query Jenkinsfile.NoKubernetesPlugin for appropriate parameter values. Manually build the Docker image and push it to a registry that is accessible from your cluster (Docker Hub, IBM Cloud Container Registry, manually deployed Quay instance): docker build -t <private-registry>/<image-namespace>/order-command-ms:latest order-command-ms/ docker build -t <private-registry>/<image-namespace>/order-query-ms:latest order-query-ms/ docker login <private-registry> docker push <private-registry>/<image-namespace>/order-command-ms:latest docker push <private-registry>/<image-namespace>/order-query-ms:latest Generate application YAMLs via helm template for both order-command and order-query : Parameters: --set image.repository=<private-registry>/<image-namespace>/<image-repository> --set image.pullSecret=<private-registry-pullsecret> (optional or set to blank) --set kafka.brokersConfigMap=<kafka brokers ConfigMap name> --set eventstreams.enabled=(true/false) ( true when connecting to Event Streams of any kind, false when connecting to Kafka directly) --set eventstreams.apikeyConfigMap=<kafka api key Secret name> --set eventstreams.truststoreRequired=(true/false) ( true when connecting to Event Streams via ICP4I) --set eventstreams.truststoreSecret=<eventstreams jks file secret name> (only used when connecting to Event Streams via ICP4I) --set eventstreams.truststorePassword=<eventstreams jks password> (only used when connecting to Event Streams via ICP4I) --set serviceAccountName=<service-account-name> --namespace <target-namespace> --output-dir <local-template-directory> Example using Event Streams via ICP4I: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set image.pullSecret = --set kafka.brokersConfigMap = es-kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = es-eventstreams-apikey --set serviceAccountName = kcontainer-runtime --set eventstreams.truststoreRequired = true --set eventstreams.truststoreSecret = es-truststore-jks --set eventstreams.truststorePassword = password --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms - Example using Event Streams hosted on IBM Cloud: helm template --set image.repository = rhos-quay.internal-network.local/browncompute/order-command-ms --set image.pullSecret = --set kafka.brokersConfigMap = kafka-brokers --set eventstreams.enabled = true --set eventstreams.apikeyConfigMap = eventstreams-apikey --set serviceAccountName = kcontainer-runtime --output-dir templates --namespace eda-pipelines-sandbox chart/ordercommandms Deploy application using oc apply : oc apply -f templates/ordercommandms/templates","title":"Deploy to OCP 3.11"},{"location":"implementation-considerations/","text":"Implementation considerations As introduced in the solution high level design note the order entity life cycle looks like in the following diagram: The order microservice supports the implementations of this life cycle, using event sourcing and CQRS pattern. With CQRS , we separate the 'write model' from the 'read model'. The Command microservice implements the 'write model' and exposes a set of REST end points for Creating Order, Updating, Deleting Order and getting Order per ID. The query service will address more complex queries to support adhoc business requirements and most likely will need to join data between different entities like the order, the containers and the voyages. So we have two Java projects to support each service implementation. Each service is packaged as container and deployable to Kubernetes. Order command microservice Order query microservice As some requirements are related to historical query, using an event approach, we need to keep all the events related to what happens to the order. Instead of implementing a complex logic with the query and command services, the event sourcing is supported by using Kafka topics. The following diagram illustrates the CQRS and event sourcing applied to the order management service. Client to the REST api, like a back end for front end app, performs a HTTP POST operation with the order data. The command generates events and persists order on its own data source. The query part is an event consumer and defines its own data projections to support the different queries: The datasource at the command level, may not be necessary, but we want to illustrate here the fact that it is possible to have a SQL based database or a document oriented database to keep the order last state: a call to get /orders/{id} will return the current order state. For the query part the projection can be kept in memory or persisted on its own data store. The decision, to go for in memory or to use a database, depends upon the amount of data to join, and the persitence time horizon set at the Kafka topic level. In case of problem or while starting, an event driven service may always rebuild its view by re-reading the topic from the beginning. Note An alternate solution is to have the BFF pushing events to the event source and then having the order service consuming event to persist them, as illustrated in the following diagram: As the BFF still needs to get order by ID or perform complex queries, it has to access the order service using HTTP, therefore we have prefered to use one communication protocol. The following sequence diagram illustrates the relationships between the components over time: To avoid transaction between the database update and the event published, the choice is to publish the event as early as it is received and use a consumer inside the command service to load the data and save to the database. The kafka topic act as a source of trust for this service. This is illustrated in this article. The /orders POST REST end point source code is here and the order events consumer in the command pattern. See the class OrderCRUDService.java . Produce order events to the orders topic. Consume events to update the state of the order or enrich it with new elements. When the application starts there is a ServletContextListener implementation class started to create a kafka consumer and to subscribe to order events (different types). When consumer reaches an issue to get event it creates an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop. Data and Event Model By applying a domain-driven design we can identify aggregates, entities, value objects and domain events. Those elements help us to be our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may look use relational database and may have information like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format pickupDate : string ; // date as ISO format } class OrderContainers { orderID : string ; containerID : string []; } On the event side we may generate OrderCreated, OrderCancelled,... But what is in the event payload? We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEvent { orderId : string ; timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. There are other questions we need to address is real project: do we need to ensure consistency between those data views? Can we consider the event, which are immutable data elements, as the source of truth? In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard. If you want to read more about the Event Sourcing and CQRS patterns see this article.","title":"Implementation considerations"},{"location":"implementation-considerations/#implementation-considerations","text":"As introduced in the solution high level design note the order entity life cycle looks like in the following diagram: The order microservice supports the implementations of this life cycle, using event sourcing and CQRS pattern. With CQRS , we separate the 'write model' from the 'read model'. The Command microservice implements the 'write model' and exposes a set of REST end points for Creating Order, Updating, Deleting Order and getting Order per ID. The query service will address more complex queries to support adhoc business requirements and most likely will need to join data between different entities like the order, the containers and the voyages. So we have two Java projects to support each service implementation. Each service is packaged as container and deployable to Kubernetes. Order command microservice Order query microservice As some requirements are related to historical query, using an event approach, we need to keep all the events related to what happens to the order. Instead of implementing a complex logic with the query and command services, the event sourcing is supported by using Kafka topics. The following diagram illustrates the CQRS and event sourcing applied to the order management service. Client to the REST api, like a back end for front end app, performs a HTTP POST operation with the order data. The command generates events and persists order on its own data source. The query part is an event consumer and defines its own data projections to support the different queries: The datasource at the command level, may not be necessary, but we want to illustrate here the fact that it is possible to have a SQL based database or a document oriented database to keep the order last state: a call to get /orders/{id} will return the current order state. For the query part the projection can be kept in memory or persisted on its own data store. The decision, to go for in memory or to use a database, depends upon the amount of data to join, and the persitence time horizon set at the Kafka topic level. In case of problem or while starting, an event driven service may always rebuild its view by re-reading the topic from the beginning. Note An alternate solution is to have the BFF pushing events to the event source and then having the order service consuming event to persist them, as illustrated in the following diagram: As the BFF still needs to get order by ID or perform complex queries, it has to access the order service using HTTP, therefore we have prefered to use one communication protocol. The following sequence diagram illustrates the relationships between the components over time: To avoid transaction between the database update and the event published, the choice is to publish the event as early as it is received and use a consumer inside the command service to load the data and save to the database. The kafka topic act as a source of trust for this service. This is illustrated in this article. The /orders POST REST end point source code is here and the order events consumer in the command pattern. See the class OrderCRUDService.java . Produce order events to the orders topic. Consume events to update the state of the order or enrich it with new elements. When the application starts there is a ServletContextListener implementation class started to create a kafka consumer and to subscribe to order events (different types). When consumer reaches an issue to get event it creates an error to the errors topic, so administrator user could replay the events from the last committed offset. Any kafka broker communication issue is shutting down the consumer loop.","title":"Implementation considerations"},{"location":"implementation-considerations/#data-and-event-model","text":"By applying a domain-driven design we can identify aggregates, entities, value objects and domain events. Those elements help us to be our information model as classes. For any event-driven microservice you need to assess what data to carry in the event and what persist in the potential data source. The following diagram illustrates the different data models in the context of this order microservice: The Order entered in the User interface is defined like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format } The information to persist in the database may be used to do analytics, and get the last status of order. It may look use relational database and may have information like: class Address { street : string ; city : string ; country : string ; state : string ; zipcode : string ; } class Order { orderID : string ; customerID : string ; pickupAddress : Address ; destinationAddress : Address ; productID : string ; quantity : string ; expectedDeliveryDate : string ; // date as ISO format pickupDate : string ; // date as ISO format } class OrderContainers { orderID : string ; containerID : string []; } On the event side we may generate OrderCreated, OrderCancelled,... But what is in the event payload? We can propose the following structure where type will help to specify the event type and by getting a generic payload we can have anything in it. class OrderEvent { orderId : string ; timestamp : string ; // date as ISO format payload : any ; type : string ; version : string ; } version attribute will be used when we will use a schema registry. There are other questions we need to address is real project: do we need to ensure consistency between those data views? Can we consider the event, which are immutable data elements, as the source of truth? In traditional SOA service with application maintaining all the tables and beans to support all the business requirements, ACID transactions support the consistency and integrity of the data, and the database is one source of truth. With microservices responsible to manage its own aggregate, clearly separated from other business entities, data eventual consistency is the standard. If you want to read more about the Event Sourcing and CQRS patterns see this article.","title":"Data and Event Model"}]}